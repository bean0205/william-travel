# Smart Web Scraper - H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng T·ª´ A ƒê·∫øn Z

## üìñ M·ª•c L·ª•c
1. [Gi·ªõi Thi·ªáu](#gi·ªõi-thi·ªáu)
2. [C√†i ƒê·∫∑t](#c√†i-ƒë·∫∑t)
3. [B·∫Øt ƒê·∫ßu Nhanh](#b·∫Øt-ƒë·∫ßu-nhanh)
4. [H∆∞·ªõng D·∫´n Chi Ti·∫øt](#h∆∞·ªõng-d·∫´n-chi-ti·∫øt)
5. [T√πy Ch·ªânh N√¢ng Cao](#t√πy-ch·ªânh-n√¢ng-cao)
6. [S·ª≠ D·ª•ng Command Line](#s·ª≠-d·ª•ng-command-line)
7. [X·ª≠ L√Ω L·ªói](#x·ª≠-l√Ω-l·ªói)
8. [FAQ](#faq)

---

## üåü Gi·ªõi Thi·ªáu

Smart Web Scraper l√† m·ªôt ·ª©ng d·ª•ng Python th√¥ng minh c√≥ kh·∫£ nƒÉng:
- **T·ª± ƒë·ªông nh·∫≠n di·ªán** lo·∫°i trang web (tin t·ª©c, blog, th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠)
- **C√†o n·ªôi dung ph√π h·ª£p** theo t·ª´ng lo·∫°i trang
- **Tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON** c√≥ c·∫•u tr√∫c
- **M·ªü r·ªông d·ªÖ d√†ng** v·ªõi c√°c extractor t√πy ch·ªânh

### ∆Øu ƒêi·ªÉm
‚úÖ Kh√¥ng c·∫ßn config ph·ª©c t·∫°p  
‚úÖ T·ª± ƒë·ªông x√°c ƒë·ªãnh n·ªôi dung quan tr·ªçng  
‚úÖ H·ªó tr·ª£ c·∫£ trang tƒ©nh v√† ƒë·ªông (JavaScript)  
‚úÖ Cache th√¥ng minh ƒë·ªÉ tƒÉng t·ªëc  
‚úÖ Interface d·ªÖ s·ª≠ d·ª•ng  

---

## üîß C√†i ƒê·∫∑t

### B∆∞·ªõc 1: Chu·∫©n B·ªã M√¥i Tr∆∞·ªùng
```bash
# T·∫°o m√¥i tr∆∞·ªùng ·∫£o (khuy·∫øn ngh·ªã)
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# ho·∫∑c: venv\Scripts\activate  # Windows

# C·∫≠p nh·∫≠t pip
pip install --upgrade pip
```

### B∆∞·ªõc 2: C√†i ƒê·∫∑t Dependencies
```bash
cd /path/to/caodulieu
pip install -r requirements.txt
```

### B∆∞·ªõc 3: Ki·ªÉm Tra C√†i ƒê·∫∑t
```bash
python -c "from smart_scraper import SmartScraper; print('‚úÖ C√†i ƒë·∫∑t th√†nh c√¥ng!')"
```

---

## üöÄ B·∫Øt ƒê·∫ßu Nhanh

### V√≠ D·ª• ƒê·∫ßu Ti√™n - 30 Gi√¢y
```python
from smart_scraper import SmartScraper

# Kh·ªüi t·∫°o scraper
scraper = SmartScraper()

# C√†o m·ªôt trang web
result = scraper.scrape("https://example.com")

# Xem k·∫øt qu·∫£
print(f"Lo·∫°i trang: {result['page_type']}")
print(f"Ti√™u ƒë·ªÅ: {result['content'].get('title', 'Kh√¥ng c√≥')}")
print(f"ƒê·ªô tin c·∫≠y: {result['extraction_confidence']:.2f}")

# ƒê√≥ng scraper
scraper.close()
```

### K·∫øt Qu·∫£ M·∫´u
```json
{
  "url": "https://example.com",
  "page_type": "unknown",
  "classification_confidence": 0.0,
  "extraction_confidence": 0.5,
  "extractor_used": "generic",
  "timestamp": "2025-05-27T21:30:00.123456",
  "content": {
    "title": "Example Domain",
    "meta_description": "This domain is for use in illustrative examples...",
    "main_content": "Example Domain. This domain is established...",
    "images": [],
    "links": ["https://www.iana.org/domains/example"],
    "headings": ["Example Domain"]
  },
  "success": true
}
```

---

## üìö H∆∞·ªõng D·∫´n Chi Ti·∫øt

### 1. Kh·ªüi T·∫°o SmartScraper

#### C·∫•u H√¨nh C∆° B·∫£n
```python
from smart_scraper import SmartScraper

# C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
scraper = SmartScraper()

# C·∫•u h√¨nh t√πy ch·ªânh
scraper = SmartScraper(
    use_selenium=False,     # True n·∫øu c·∫ßn x·ª≠ l√Ω JavaScript
    headless=True,          # False ƒë·ªÉ hi·ªán tr√¨nh duy·ªát (khi d√πng Selenium)
    timeout=30,             # Timeout cho m·ªói request (gi√¢y)
    max_retries=3           # S·ªë l·∫ßn th·ª≠ l·∫°i khi l·ªói
)
```

### 2. C√†o D·ªØ Li·ªáu C∆° B·∫£n

#### C√†o M·ªôt URL
```python
# C√†o t·ª± ƒë·ªông (nh·∫≠n di·ªán lo·∫°i trang)
result = scraper.scrape("https://news-website.com")

# C√†o v·ªõi lo·∫°i trang c·ª• th·ªÉ
result = scraper.scrape("https://blog.com", force_type="blog")

# C√†o kh√¥ng d√πng cache
result = scraper.scrape("https://example.com", use_cache=False)
```

#### Ch·ªâ Nh·∫≠n Di·ªán Lo·∫°i Trang
```python
classification = scraper.classify_page("https://example.com")
print(f"Lo·∫°i trang: {classification['page_type']}")
print(f"ƒê·ªô tin c·∫≠y: {classification['confidence']}")
```

### 3. C√†o Nhi·ªÅu URL

```python
urls = [
    "https://news-site.com/article1",
    "https://blog.com/post1",
    "https://shop.com/product1"
]

# C√†o tu·∫ßn t·ª± v·ªõi delay
results = scraper.scrape_multiple(urls, delay=1.0)

for result in results:
    if result['success']:
        print(f"‚úÖ {result['url']}: {result['page_type']}")
    else:
        print(f"‚ùå {result['url']}: {result.get('error', 'Unknown error')}")
```

### 4. Hi·ªÉu K·∫øt Qu·∫£ Tr·∫£ V·ªÅ

#### C·∫•u Tr√∫c JSON Chu·∫©n
```python
result = {
    "url": "URL g·ªëc",
    "page_type": "news|blog|ecommerce|unknown",
    "classification_confidence": 0.95,      # ƒê·ªô tin c·∫≠y nh·∫≠n di·ªán (0-1)
    "extraction_confidence": 0.87,         # ƒê·ªô tin c·∫≠y tr√≠ch xu·∫•t (0-1)
    "extractor_used": "news",               # Extractor ƒë√£ s·ª≠ d·ª•ng
    "timestamp": "2025-05-27T21:30:00Z",
    "content": {
        # N·ªôi dung t√πy theo lo·∫°i trang
    },
    "html_analysis": {
        "structure_info": {},
        "content_score": 0.8,
        "metadata": {}
    },
    "success": true,
    "from_cache": false
}
```

#### N·ªôi Dung Theo Lo·∫°i Trang

**Trang Tin T·ª©c (News):**
```json
"content": {
    "title": "Ti√™u ƒë·ªÅ b√†i b√°o",
    "author": "T√™n t√°c gi·∫£",
    "publish_date": "2025-05-27",
    "main_content": "N·ªôi dung ch√≠nh c·ªßa b√†i b√°o...",
    "summary": "T√≥m t·∫Øt ng·∫Øn",
    "images": ["url1.jpg", "url2.jpg"],
    "tags": ["tag1", "tag2"],
    "category": "Th·ªÉ thao"
}
```

**Trang Blog:**
```json
"content": {
    "title": "Ti√™u ƒë·ªÅ b√†i vi·∫øt",
    "author": "T√°c gi·∫£ blog",
    "publish_date": "2025-05-27",
    "main_content": "N·ªôi dung b√†i vi·∫øt...",
    "excerpt": "ƒêo·∫°n m·ªü ƒë·∫ßu",
    "images": ["featured.jpg"],
    "tags": ["travel", "tips"],
    "comments_count": 15
}
```

**Trang Th∆∞∆°ng M·∫°i ƒêi·ªán T·ª≠:**
```json
"content": {
    "product_name": "T√™n s·∫£n ph·∫©m",
    "price": "1,000,000 VND",
    "description": "M√¥ t·∫£ s·∫£n ph·∫©m...",
    "images": ["product1.jpg", "product2.jpg"],
    "availability": "C√≤n h√†ng",
    "rating": "4.5/5",
    "reviews_count": 120,
    "specifications": {
        "brand": "Brand Name",
        "model": "Model ABC"
    }
}
```

---

## üîß T√πy Ch·ªânh N√¢ng Cao

### 1. T·∫°o Extractor T√πy Ch·ªânh

#### B∆∞·ªõc 1: T·∫°o Class Extractor
```python
from smart_scraper.extractors import BaseExtractor

class ForumExtractor(BaseExtractor):
    def __init__(self):
        super().__init__()
        self.extractor_type = "forum"
    
    def extract(self, soup, url=""):
        """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ trang forum"""
        title = self._extract_title(soup)
        posts = self._extract_posts(soup)
        users = self._extract_users(soup)
        
        return {
            "title": title,
            "posts": posts,
            "users_online": users,
            "topics_count": len(posts)
        }
    
    def get_confidence_score(self, soup):
        """T√≠nh ƒë·ªô tin c·∫≠y c·ªßa extractor n√†y v·ªõi trang hi·ªán t·∫°i"""
        score = 0
        
        # Ki·ªÉm tra c√°c y·∫øu t·ªë ƒë·∫∑c tr∆∞ng c·ªßa forum
        if soup.find(class_="forum-post"):
            score += 0.3
        if soup.find(class_="user-info"):
            score += 0.2
        if "forum" in soup.get_text().lower():
            score += 0.3
        if soup.find_all("div", class_="topic"):
            score += 0.2
            
        return min(score, 1.0)
    
    def _extract_title(self, soup):
        """Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ trang"""
        title_tag = soup.find('title')
        return title_tag.get_text().strip() if title_tag else ""
    
    def _extract_posts(self, soup):
        """Tr√≠ch xu·∫•t danh s√°ch b√†i vi·∫øt"""
        posts = []
        post_elements = soup.find_all(class_="forum-post")
        
        for post in post_elements:
            posts.append({
                "content": post.get_text().strip(),
                "author": self._extract_post_author(post)
            })
        
        return posts
    
    def _extract_users(self, soup):
        """Tr√≠ch xu·∫•t th√¥ng tin users"""
        users_element = soup.find(class_="users-online")
        if users_element:
            return users_element.get_text().strip()
        return "0"
    
    def _extract_post_author(self, post_element):
        """Tr√≠ch xu·∫•t t√°c gi·∫£ c·ªßa m·ªôt b√†i post"""
        author_element = post_element.find(class_="author")
        return author_element.get_text().strip() if author_element else "Unknown"
```

#### B∆∞·ªõc 2: S·ª≠ D·ª•ng Extractor T√πy Ch·ªânh
```python
# Kh·ªüi t·∫°o scraper
scraper = SmartScraper()

# Th√™m extractor t√πy ch·ªânh
forum_extractor = ForumExtractor()
scraper.add_extractor("forum", forum_extractor)

# S·ª≠ d·ª•ng extractor m·ªõi
result = scraper.scrape("https://forum-website.com", force_type="forum")

print(f"Lo·∫°i trang: {result['page_type']}")  # "forum"
print(f"S·ªë b√†i vi·∫øt: {result['content']['topics_count']}")
```

### 2. C·∫•u H√¨nh Selenium cho Trang ƒê·ªông

```python
# Kh·ªüi t·∫°o v·ªõi Selenium
scraper = SmartScraper(
    use_selenium=True,
    headless=True,          # False ƒë·ªÉ debug
    timeout=60              # Timeout l·ªõn h∆°n cho JS
)

# C√†o trang c√≥ JavaScript
result = scraper.scrape("https://spa-website.com")

# Selenium s·∫Ω ƒë·ª£i JS load xong r·ªìi m·ªõi c√†o
print(f"N·ªôi dung: {result['content']}")

scraper.close()  # Quan tr·ªçng: ph·∫£i close ƒë·ªÉ gi·∫£i ph√≥ng browser
```

### 3. Qu·∫£n L√Ω Cache

```python
# Xem th·ªëng k√™ cache
stats = scraper.cache.get_stats() if scraper.cache else {}
print(f"S·ªë entries: {stats.get('total_entries', 0)}")
print(f"Dung l∆∞·ª£ng: {stats.get('total_size_mb', 0):.2f} MB")

# X√≥a cache c≈© (h·∫øt h·∫°n)
expired_count = scraper.cleanup_cache()
print(f"ƒê√£ x√≥a {expired_count} entries h·∫øt h·∫°n")

# X√≥a to√†n b·ªô cache
total_cleared = scraper.clear_cache()
print(f"ƒê√£ x√≥a {total_cleared} entries")
```

### 4. C·∫•u H√¨nh Logging

```python
import logging
from smart_scraper.logging_config import setup_logging

# C·∫•u h√¨nh logging chi ti·∫øt
setup_logging(
    level=logging.DEBUG,        # DEBUG, INFO, WARNING, ERROR
    log_to_file=True,          # Ghi v√†o file
    log_filename="my_scraper.log"
)

# Scraper s·∫Ω t·ª± ƒë·ªông log c√°c ho·∫°t ƒë·ªông
scraper = SmartScraper()
result = scraper.scrape("https://example.com")

# Ki·ªÉm tra log file ƒë·ªÉ debug
```

---

## üíª S·ª≠ D·ª•ng Command Line

Smart Web Scraper cung c·∫•p CLI m·∫°nh m·∫Ω cho vi·ªác s·ª≠ d·ª•ng nhanh.

### 1. C√†o URL ƒê∆°n Gi·∫£n
```bash
python cli.py https://example.com
```

### 2. L∆∞u K·∫øt Qu·∫£ Ra File
```bash
# JSON format (m·∫∑c ƒë·ªãnh)
python cli.py https://news-site.com --output results.json

# CSV format
python cli.py https://blog.com --format csv --output blog_data.csv

# XML format
python cli.py https://shop.com --format xml --output products.xml
```

### 3. Ch·ªâ Nh·∫≠n Di·ªán Lo·∫°i Trang
```bash
python cli.py https://example.com --classify-only
```

### 4. S·ª≠ D·ª•ng Selenium
```bash
python cli.py https://spa-website.com --selenium --timeout 60
```

### 5. Qu·∫£n L√Ω Cache
```bash
# Xem th·ªëng k√™ cache
python cli.py --cache-stats

# X√≥a to√†n b·ªô cache
python cli.py --clear-cache

# X√≥a cache h·∫øt h·∫°n
python cli.py --cleanup-cache
```

### 6. X·ª≠ L√Ω Batch t·ª´ File
```bash
# T·∫°o file URLs
echo "https://vnexpress.net/" > urls.txt
echo "https://dantri.com.vn/" >> urls.txt
echo "https://tuoitre.vn/" >> urls.txt

# X·ª≠ l√Ω batch c∆° b·∫£n
python cli.py --file urls.txt

# Batch v·ªõi c·∫•u h√¨nh n√¢ng cao
python cli.py --file urls.txt \
    --delay 2 \
    --format json \
    --output batch_results.json \
    --verbose

# Batch v·ªõi Selenium v√† kh√¥ng cache
python cli.py --file urls.txt \
    --selenium \
    --no-cache \
    --delay 3 \
    --timeout 30
```

### 7. C·∫•u H√¨nh N√¢ng Cao
```bash
python cli.py https://example.com \
    --timeout 45 \
    --retries 5 \
    --delay 1 \
    --force-type news \
    --no-cache \
    --verbose \
    --output detailed_result.json
```

### 8. V√≠ D·ª• Th·ª±c T·∫ø
```bash
# C√†o tin t·ª©c v√† l∆∞u CSV
python cli.py https://vnexpress.net/tin-tuc-moi \
    --format csv \
    --output news_$(date +%Y%m%d).csv \
    --verbose

# C√†o s·∫£n ph·∫©m e-commerce v·ªõi Selenium
python cli.py https://shopee.vn/product/123 \
    --selenium \
    --timeout 30 \
    --format json \
    --output product_info.json

# Batch processing nhi·ªÅu trang tin t·ª©c
python cli.py --file news_urls.txt \
    --delay 2 \
    --format csv \
    --output all_news.csv \
    --force-type news \
    --verbose
```

---

## üìã CLI Reference - Tham Kh·∫£o C√°c Options

### C√∫ Ph√°p C∆° B·∫£n
```bash
python cli.py [URL|--file FILE] [OPTIONS]
```

### Tham S·ªë B·∫Øt Bu·ªôc (ch·ªçn 1 trong 2)
- `URL` - URL trang web c·∫ßn c√†o
- `--file FILE, -f FILE` - File ch·ª©a danh s√°ch URLs (m·ªôt URL m·ªói d√≤ng)

### Options ƒê·∫ßu Ra
- `--output FILE, -o FILE` - L∆∞u k·∫øt qu·∫£ v√†o file
- `--format FORMAT` - ƒê·ªãnh d·∫°ng ƒë·∫ßu ra: `json` (m·∫∑c ƒë·ªãnh), `csv`, `xml`

### Options C·∫•u H√¨nh
- `--timeout SECONDS` - Timeout cho m·ªói request (m·∫∑c ƒë·ªãnh: 30s)
- `--retries COUNT` - S·ªë l·∫ßn th·ª≠ l·∫°i khi th·∫•t b·∫°i (m·∫∑c ƒë·ªãnh: 3)
- `--delay SECONDS` - ƒê·ªô tr·ªÖ gi·ªØa c√°c requests (m·∫∑c ƒë·ªãnh: 0s)

### Options Selenium
- `--selenium` - S·ª≠ d·ª•ng Selenium WebDriver
- `--headless` - Ch·∫°y browser ·∫©n (m·∫∑c ƒë·ªãnh khi d√πng Selenium)
- `--no-headless` - Hi·ªÉn th·ªã browser (ƒë·ªÉ debug)

### Options Cache
- `--no-cache` - B·ªè qua cache, lu√¥n c√†o t·ª´ web
- `--cache-stats` - Hi·ªÉn th·ªã th·ªëng k√™ cache
- `--clear-cache` - X√≥a to√†n b·ªô cache
- `--cleanup-cache` - X√≥a cache h·∫øt h·∫°n

### Options Kh√°c
- `--classify-only` - Ch·ªâ ph√¢n lo·∫°i trang, kh√¥ng c√†o n·ªôi dung
- `--force-type TYPE` - √âp ki·ªÉu trang: `news`, `blog`, `ecommerce`, `general`
- `--verbose, -v` - Hi·ªÉn th·ªã log chi ti·∫øt
- `--help, -h` - Hi·ªÉn th·ªã help

### V√≠ D·ª• Options K·∫øt H·ª£p
```bash
# C√†o v·ªõi ƒë·∫ßy ƒë·ªß options
python cli.py https://example.com \
    --selenium \
    --timeout 60 \
    --retries 5 \
    --delay 2 \
    --no-cache \
    --format json \
    --output result.json \
    --verbose

# Batch processing v·ªõi rate limiting
python cli.py --file urls.txt \
    --delay 3 \
    --timeout 45 \
    --format csv \
    --output batch_results.csv \
    --force-type news \
    --verbose
```

---

## üõ†Ô∏è X·ª≠ L√Ω L·ªói

### 1. C√°c Lo·∫°i L·ªói Th∆∞·ªùng G·∫∑p

#### L·ªói K·∫øt N·ªëi
```python
result = scraper.scrape("https://invalid-domain.com")
if not result['success']:
    print(f"L·ªói: {result['error']}")
    # "Scraping error: HTTPSConnectionPool..."
```

#### L·ªói Timeout
```python
# TƒÉng timeout cho trang load ch·∫≠m
scraper = SmartScraper(timeout=60)
result = scraper.scrape("https://slow-website.com")
```

#### L·ªói Selenium
```python
try:
    scraper = SmartScraper(use_selenium=True)
    result = scraper.scrape("https://example.com")
except Exception as e:
    print(f"Selenium error: {e}")
    # Th·ª≠ fallback v·ªÅ requests
    scraper_fallback = SmartScraper(use_selenium=False)
    result = scraper_fallback.scrape("https://example.com")
```

### 2. Best Practices X·ª≠ L√Ω L·ªói

```python
def safe_scrape(url):
    """H√†m c√†o an to√†n v·ªõi x·ª≠ l√Ω l·ªói ƒë·∫ßy ƒë·ªß"""
    scraper = None
    try:
        scraper = SmartScraper(timeout=30, max_retries=3)
        result = scraper.scrape(url)
        
        if result['success']:
            return result
        else:
            print(f"‚ùå Kh√¥ng th·ªÉ c√†o {url}: {result.get('error')}")
            return None
            
    except Exception as e:
        print(f"‚ùå L·ªói b·∫•t ng·ªù khi c√†o {url}: {e}")
        return None
    
    finally:
        if scraper:
            scraper.close()

# S·ª≠ d·ª•ng
result = safe_scrape("https://example.com")
if result:
    print(f"‚úÖ Th√†nh c√¥ng: {result['page_type']}")
```

### 3. Retry Logic T√πy Ch·ªânh

```python
import time
import random

def scrape_with_custom_retry(url, max_attempts=5):
    """C√†o v·ªõi retry logic t√πy ch·ªânh"""
    scraper = SmartScraper()
    
    for attempt in range(max_attempts):
        try:
            result = scraper.scrape(url, use_cache=False)
            if result['success']:
                return result
            
            # Exponential backoff v·ªõi jitter
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            print(f"Th·ª≠ l·∫°i sau {wait_time:.1f} gi√¢y... (l·∫ßn {attempt + 1})")
            time.sleep(wait_time)
            
        except Exception as e:
            print(f"L·ªói l·∫ßn {attempt + 1}: {e}")
            if attempt == max_attempts - 1:
                raise
    
    scraper.close()
    return None
```

---

## ‚ùì FAQ - C√¢u H·ªèi Th∆∞·ªùng G·∫∑p

### Q1: L√†m sao ƒë·ªÉ c√†o trang web c·∫ßn ƒëƒÉng nh·∫≠p?
**A:** S·ª≠ d·ª•ng Selenium v√† t·ª± ƒë·ªông h√≥a ƒëƒÉng nh·∫≠p:

```python
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

scraper = SmartScraper(use_selenium=True, headless=False)

# Access driver tr·ª±c ti·∫øp ƒë·ªÉ ƒëƒÉng nh·∫≠p
driver = scraper.driver
driver.get("https://website.com/login")

# T·ª± ƒë·ªông ƒëƒÉng nh·∫≠p
username_input = driver.find_element(By.NAME, "username")
password_input = driver.find_element(By.NAME, "password")

username_input.send_keys("your_username")
password_input.send_keys("your_password")
driver.find_element(By.XPATH, "//button[@type='submit']").click()

# ƒê·ª£i ƒëƒÉng nh·∫≠p xong
WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, "dashboard"))
)

# Gi·ªù c√≥ th·ªÉ c√†o trang protected
result = scraper.scrape("https://website.com/protected-page")
```

### Q2: L√†m sao ƒë·ªÉ c√†o nhi·ªÅu trang m·ªôt c√°ch hi·ªáu qu·∫£?
**A:** S·ª≠ d·ª•ng threading v√† batch processing:

```python
import threading
from concurrent.futures import ThreadPoolExecutor
import time

def scrape_batch(urls, max_workers=5, delay=1.0):
    """C√†o nhi·ªÅu URL song song"""
    results = []
    
    def scrape_single(url):
        scraper = SmartScraper()
        try:
            result = scraper.scrape(url)
            time.sleep(delay)  # Rate limiting
            return result
        finally:
            scraper.close()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(scrape_single, url) for url in urls]
        results = [future.result() for future in futures]
    
    return results

# S·ª≠ d·ª•ng
urls = ["https://site1.com", "https://site2.com", "https://site3.com"]
results = scrape_batch(urls, max_workers=3, delay=2.0)
```

### Q3: Cache ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?
**A:** Cache t·ª± ƒë·ªông l∆∞u k·∫øt qu·∫£ ƒë·ªÉ tƒÉng t·ªëc:

```python
# L·∫ßn ƒë·∫ßu: c√†o t·ª´ web (ch·∫≠m)
result1 = scraper.scrape("https://example.com")  # ~2 gi√¢y

# L·∫ßn sau: l·∫•y t·ª´ cache (nhanh)
result2 = scraper.scrape("https://example.com")  # ~0.01 gi√¢y
print(result2['from_cache'])  # True

# B·ªè qua cache n·∫øu c·∫ßn data m·ªõi
result3 = scraper.scrape("https://example.com", use_cache=False)
```

### Q4: L√†m sao ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c?
**A:** V√†i tips ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c:

```python
# 1. S·ª≠ d·ª•ng Selenium cho trang c√≥ JS
scraper = SmartScraper(use_selenium=True)

# 2. TƒÉng timeout cho trang load ch·∫≠m
scraper = SmartScraper(timeout=60)

# 3. Force type n·∫øu bi·∫øt tr∆∞·ªõc lo·∫°i trang
result = scraper.scrape("https://blog.com", force_type="blog")

# 4. Ki·ªÉm tra confidence score
if result['extraction_confidence'] < 0.7:
    print("‚ö†Ô∏è K·∫øt qu·∫£ c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c")

# 5. T·∫°o extractor t√πy ch·ªânh cho domain c·ª• th·ªÉ
class MyNewsExtractor(BaseExtractor):
    # Custom logic cho trang tin t·ª©c c·ª• th·ªÉ
    pass
```

### Q5: C√°ch debug khi c√≥ v·∫•n ƒë·ªÅ?
**A:** S·ª≠ d·ª•ng logging v√† debug mode:

```python
import logging
from smart_scraper.logging_config import setup_logging

# B·∫≠t debug logging
setup_logging(level=logging.DEBUG, log_to_file=True)

# S·ª≠ d·ª•ng Selenium kh√¥ng headless ƒë·ªÉ xem browser
scraper = SmartScraper(use_selenium=True, headless=False)

# Ki·ªÉm tra HTML th√¥
result = scraper.scrape("https://example.com")
print("HTML Analysis:", result['html_analysis'])

# Ki·ªÉm tra confidence scores
print(f"Classification: {result['classification_confidence']}")
print(f"Extraction: {result['extraction_confidence']}")
```

### Q6: C√≥ th·ªÉ c√†o d·ªØ li·ªáu t·ª´ API kh√¥ng?
**A:** Smart Scraper chuy√™n cho HTML, nh∆∞ng c√≥ th·ªÉ m·ªü r·ªông:

```python
class APIExtractor(BaseExtractor):
    def __init__(self):
        super().__init__()
        self.extractor_type = "api"
    
    def extract(self, soup, url=""):
        # Thay v√¨ parse HTML, g·ªçi API
        import requests
        api_url = url.replace('/page/', '/api/')
        response = requests.get(api_url)
        return response.json()
    
    def get_confidence_score(self, soup):
        # Check if this looks like an API endpoint
        return 0.9 if "/api/" in soup.get_text() else 0.0

# S·ª≠ d·ª•ng
scraper.add_extractor("api", APIExtractor())
result = scraper.scrape("https://api.example.com/data", force_type="api")
```

---

## üéØ K·∫øt Lu·∫≠n

Smart Web Scraper cung c·∫•p gi·∫£i ph√°p ho√†n ch·ªânh cho vi·ªác c√†o d·ªØ li·ªáu web th√¥ng minh:

### ‚úÖ T√≠nh NƒÉng N·ªïi B·∫≠t
- **T·ª± ƒë·ªông nh·∫≠n di·ªán** lo·∫°i trang web
- **Tr√≠ch xu·∫•t th√¥ng minh** n·ªôi dung quan tr·ªçng  
- **M·ªü r·ªông d·ªÖ d√†ng** v·ªõi custom extractors
- **Performance cao** v·ªõi caching v√† retry logic
- **D·ªÖ s·ª≠ d·ª•ng** v·ªõi c·∫£ API v√† CLI

### üõ°Ô∏è Best Practices
1. **Lu√¥n close scraper** sau khi s·ª≠ d·ª•ng (ƒë·∫∑c bi·ªát v·ªõi Selenium)
2. **S·ª≠ d·ª•ng timeout h·ª£p l√Ω** t√πy theo trang web
3. **Ki·ªÉm tra confidence score** ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng
4. **Implement retry logic** cho ·ª©ng d·ª•ng production
5. **T√¥n tr·ªçng robots.txt** v√† rate limiting

### üöÄ S·∫µn S√†ng S·ª≠ D·ª•ng
B√¢y gi·ªù b·∫°n ƒë√£ c√≥ ƒë·∫ßy ƒë·ªß ki·∫øn th·ª©c ƒë·ªÉ s·ª≠ d·ª•ng Smart Web Scraper hi·ªáu qu·∫£. H√£y b·∫Øt ƒë·∫ßu v·ªõi c√°c v√≠ d·ª• ƒë∆°n gi·∫£n v√† d·∫ßn d·∫ßn kh√°m ph√° c√°c t√≠nh nƒÉng n√¢ng cao!

**Ch√∫c b·∫°n c√†o d·ªØ li·ªáu th√†nh c√¥ng!** üéâ

---

*T√†i li·ªáu n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t l·∫ßn cu·ªëi: 27/05/2025*
